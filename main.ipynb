{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "\n",
    "print(\"Packages imported & ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_USERNAME = \"\"                 # can be blank if key-only tokens work\n",
    "KAGGLE_KEY      = \"\"\n",
    "\n",
    "kcfg_dir = os.path.abspath(\"./kaggle\")        # local, user-space directory\n",
    "os.makedirs(kcfg_dir, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(kcfg_dir, \"kaggle.json\"), \"w\") as f:\n",
    "    json.dump({\"username\": KAGGLE_USERNAME, \"key\": KAGGLE_KEY}, f)\n",
    "\n",
    "os.environ[\"KAGGLE_CONFIG_DIR\"] = kcfg_dir    # tells the CLI where to look\n",
    "print(\"kaggle.json written â†’\", kcfg_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "path = kagglehub.dataset_download(\"thedevastator/unlock-profits-with-e-commerce-sales-data\")\n",
    "\n",
    "print(\"Download finished. Contents of ./data:\")\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "print(path)\n",
    "\n",
    "# Move the entire downloaded folder to ./data (rename/move as 'data')\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n",
    "shutil.move(path, data_dir)\n",
    "\n",
    "print(\"Folder moved and renamed to ./data\")\n",
    "print(os.listdir(data_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f08dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(data_dir, \"Amazon Sale Report.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f246ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnamed or irrelevant columns\n",
    "df.drop(columns=[\n",
    "    'Unnamed: 22', 'promotion-ids', 'Courier Status', 'ship-city',\n",
    "    'ship-postal-code', 'ship-country'\n",
    "], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6574ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize column names\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('-', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac258925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only completed sales\n",
    "shipped_df = df[(df['status'].str.lower() == 'shipped') | (df['status'].str.lower() == 'shipped - delivered to buyer')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96804633",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipped_df['sku'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e126d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing or invalid sales\n",
    "shipped_df.dropna(subset=['amount', 'qty', 'sku'], inplace=True)\n",
    "\n",
    "# Remove rows with non-positive amount or quantity\n",
    "shipped_df = shipped_df[(shipped_df['amount'] > 0) & (shipped_df['qty'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipped_df['date'] = pd.to_datetime(shipped_df['date'], errors='coerce')\n",
    "shipped_df['qty'] = pd.to_numeric(shipped_df['qty'], errors='coerce')\n",
    "shipped_df['amount'] = pd.to_numeric(shipped_df['amount'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid dates\n",
    "shipped_df = shipped_df.dropna(subset=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982c21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipped_df['unit_price'] = shipped_df['amount'] / shipped_df['qty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipped_df['revenue'] = shipped_df['amount']  # already available, but explicit naming helps clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6921401",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipped_df['year'] = shipped_df['date'].dt.year\n",
    "shipped_df['month'] = shipped_df['date'].dt.month\n",
    "shipped_df['day'] = shipped_df['date'].dt.day\n",
    "shipped_df['weekday'] = shipped_df['date'].dt.day_name()\n",
    "shipped_df['weekofyear'] = shipped_df['date'].dt.isocalendar().week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54621ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipped_df['sales_channel'] = shipped_df['sales_channel'].str.strip().str.lower()\n",
    "shipped_df['fulfilled_by'] = shipped_df['fulfilled_by'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_agg = shipped_df.groupby('sku').agg({\n",
    "    'unit_price': 'mean',\n",
    "    'qty': 'sum',\n",
    "    'revenue': 'sum',\n",
    "    'style': 'first',\n",
    "    'category': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename for clarity\n",
    "sku_agg.columns = ['sku', 'avg_price', 'total_qty', 'total_revenue', 'style', 'category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2ebbb",
   "metadata": {},
   "source": [
    "### Preliminary EDA Highlights (based on features)\n",
    "\n",
    "Weâ€™ll analyze:\n",
    "* Price distribution (identify pricing tiers or outliers)\n",
    "* Demand vs price (scatter plots or binning)\n",
    "* Top categories/SKUs by revenue\n",
    "* Seasonality using time-based aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price Distribution\n",
    "sns.histplot(shipped_df['unit_price'], bins=50, kde=True)\n",
    "plt.title(\"Unit Price Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74bbe9f",
   "metadata": {},
   "source": [
    "### Key Observations:\n",
    "\n",
    "1. **Right-skewed distribution**:\n",
    "\n",
    "   * The unit prices are **concentrated between â‚¹300 and â‚¹1000**.\n",
    "   * This tells us that **most products are low to mid-range priced**.\n",
    "   * A long tail exists where a few products are priced as high as â‚¹2500.\n",
    "\n",
    "2. **Peak (mode) around â‚¹400â€“â‚¹600**:\n",
    "\n",
    "   * The **most frequent unit price range** is roughly in the â‚¹400â€“â‚¹600 band.\n",
    "   * This is the pricing \"sweet spot\" where your product volume is highest.\n",
    "\n",
    "3. **Multiple peaks (multimodal)**:\n",
    "\n",
    "   * There are **several smaller spikes** after â‚¹800, around â‚¹1000 and â‚¹1200.\n",
    "   * This may indicate **distinct product segments** or categories with different pricing strategies (e.g., basic, premium SKUs).\n",
    "\n",
    "4. **KDE line (blue curve)**:\n",
    "\n",
    "   * This smoothed line (Kernel Density Estimate) helps visualize the **underlying probability density**.\n",
    "   * It confirms the **non-normal**, skewed nature of your price distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f72c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipped_df['sku'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76619dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sku=\"JNE3797-KR-L\"\n",
    "sku_data = shipped_df[shipped_df['sku'] == sku]\n",
    "\n",
    "price_demand = sku_data.groupby('unit_price')['qty'].sum().reset_index()\n",
    "\n",
    "# plot demand curve\n",
    "sns.lineplot(x='unit_price', y='qty', data=price_demand, marker='o')\n",
    "plt.title(f\"Demand Curve for SKU: {sku}\")\n",
    "plt.xlabel(\"Unit Price\")\n",
    "plt.ylabel(\"Total Quantity Sold\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0763f8a",
   "metadata": {},
   "source": [
    "### ðŸ” Key Observations:\n",
    "\n",
    "1. **Demand increases up to a certain price (\\~â‚¹735)**:\n",
    "\n",
    "   * There is a **sharp rise in quantity sold as the price moves from â‚¹720 to â‚¹735**.\n",
    "   * This suggests the product may have been underpriced earlier, and customers responded more favorably at slightly higher prices (could be perceived value, offer period, or availability).\n",
    "\n",
    "2. **Demand drops after â‚¹735**:\n",
    "\n",
    "   * After â‚¹735, total quantity sold **declines steadily**, showing typical **price elasticity**: as the price increases, fewer units are sold.\n",
    "   * This is the expected economic behavior.\n",
    "\n",
    "3. **Non-linear relationship**:\n",
    "\n",
    "   * The relationship between price and demand is **not purely linear**.\n",
    "   * This implies that we might need to **fit a non-linear or log-log regression model** to capture the elasticity accurately.\n",
    "\n",
    "---\n",
    "\n",
    "### Business Insight:\n",
    "\n",
    "* The **peak demand occurs at â‚¹735**, which could be close to the **optimal price point** for maximizing volume sales of this SKU.\n",
    "* Raising prices beyond this leads to **reduced demand**, so if your margin per unit isnâ€™t high, revenue or profit may decline.\n",
    "* Lower prices (â‚¹715â€“â‚¹725) didnâ€™t help sell significantly more units either, meaning **aggressive discounting may not be necessary**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e03ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by unit price and aggregate total quantity sold\n",
    "sku_price_demand = shipped_df.groupby(['sku', 'unit_price'])['qty'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_price_demand['log_price'] = np.log(sku_price_demand['unit_price'])\n",
    "sku_price_demand['log_qty'] = np.log(sku_price_demand['qty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f38252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "elasticity_results = []\n",
    "\n",
    "for sku_id, group in sku_price_demand.groupby('sku'):\n",
    "    if group.shape[0] >= 3:  # minimum points for regression\n",
    "        X = group[['log_price']]\n",
    "        y = group['log_qty']\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        elasticity = model.coef_[0]\n",
    "        r2 = model.score(X, y)\n",
    "        elasticity_results.append({'sku': sku_id, 'elasticity': elasticity, 'r2': r2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f577da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticity_df = pd.DataFrame(elasticity_results)\n",
    "elasticity_df.sort_values(by='elasticity', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b60f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticity_df['elasticity_range'] = pd.cut(\n",
    "    elasticity_df['elasticity'],\n",
    "    bins=[-np.inf, -1, 0, np.inf],\n",
    "    labels=['Elastic', 'Inelastic', 'Positive Elasticity']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticity_df['elasticity_range'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de071bee",
   "metadata": {},
   "source": [
    "| Elasticity Range                 | Interpretation                                                        | Example SKU                  |\n",
    "| -------------------------------- | --------------------------------------------------------------------- | ---------------------------- |\n",
    "| **Elastic** (< -1)               | Demand drops sharply with price â†‘                                     | SAR062, J012-TP-XL           |\n",
    "| **Inelastic** (between 0 and -1) | Demand mildly sensitive                                               | MEN5027-KR-XXL,MEN5027-KR-XXL |\n",
    "| **Positive Elasticity**          | Counterintuitive â€“ possible data error, bundling, or premium behavior | SET257-KR-PP-M (175.8)       |\n",
    "\n",
    "---\n",
    "\n",
    "| Elasticity Range        | Count | % Share     | Interpretation                                                                                                |\n",
    "| ----------------------- | ----- | ----------- | ------------------------------------------------------------------------------------------------------------- |\n",
    "| **Elastic**             | 1306  | **\\~45%**   | Majority of SKUs â€” lowering price will likely increase total revenue. Important for discounts or price drops. |\n",
    "| **Positive Elasticity** | 1237  | **\\~42.6%** | Large chunk with unexpected behavior â€” needs inspection. Could be premium SKUs or data anomalies.             |\n",
    "| **Inelastic**           | 211   | **\\~7.3%**  | Small subset â€” price changes have limited impact. Good candidates for price increase.                         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19561196",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipped_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f130499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Group by category and unit_price, summing quantities\n",
    "cat_price_demand = shipped_df.groupby(['category', 'unit_price'])['qty'].sum().reset_index()\n",
    "\n",
    "# 2. Filter out zero or negative values\n",
    "cat_price_demand = cat_price_demand[(cat_price_demand['unit_price'] > 0) & (cat_price_demand['qty'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_price_demand['log_price'] = np.log(cat_price_demand['unit_price'])\n",
    "cat_price_demand['log_qty']   = np.log(cat_price_demand['qty'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72534373",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for cat, group in cat_price_demand.groupby('category'):\n",
    "    # Need at least 3 unique price points to fit a model\n",
    "    if group['unit_price'].nunique() < 3:\n",
    "        continue\n",
    "\n",
    "    X = group[['log_price']]\n",
    "    y = group['log_qty']\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    elasticity = model.coef_[0]\n",
    "    r2          = model.score(X, y)\n",
    "    n_points    = group.shape[0]\n",
    "\n",
    "    results.append({\n",
    "        'category':       cat,\n",
    "        'elasticity':     elasticity,\n",
    "        'r_squared':      r2,\n",
    "        'n_price_points': n_points\n",
    "    })\n",
    "\n",
    "category_elasticity_df = pd.DataFrame(results).sort_values('elasticity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cdcf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_elasticity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d6add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_elasticity_df['elasticity_range'] = pd.cut(\n",
    "    category_elasticity_df['elasticity'],\n",
    "    bins=[-np.inf, -1, 0, np.inf],\n",
    "    labels=['Elastic', 'Inelastic', 'Positive Elasticity']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdec0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_elasticity_df.value_counts('elasticity_range')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c912415",
   "metadata": {},
   "source": [
    "## What This Tells Us\n",
    "\n",
    "1. **No â€œElasticâ€ Categories (< â€“1)**\n",
    "\n",
    "   * At the **category level**, none of your product groups behaves with high price sensitivity.\n",
    "   * **Implication**: Broad price cuts are unlikely to drive large volume gains in any category.\n",
    "\n",
    "2. **Predominantly Inelastic Demand (5/8 Categories)**\n",
    "\n",
    "   * Categories like **Saree, Kurta, Bottom, Set, Ethnic Dress** have elasticities between â€“1 and 0.\n",
    "   * **Interpretation**: Customers are relatively **insensitive** to small price changes here.\n",
    "   * **Strategy**: You could **raise prices** modestly in these categories to improve margins without sacrificing too much volume.\n",
    "\n",
    "3. **Counterâ€‘Intuitive â€œPositive Elasticityâ€ (3/8 Categories)**\n",
    "\n",
    "   * **Top, Blouse, Western Dress** show a **positive** relationship: price â†‘ â†’ demand â†‘.\n",
    "   * **Possible Explanations**:\n",
    "\n",
    "     * **Premium signaling**: Higher price suggests higher quality, attracting more buyers.\n",
    "     * **Data artifacts**: Bundles, promotions, or inventory lulls at lower price points distorting the curve.\n",
    "   * **Action**: Investigate furtherâ€”check for:\n",
    "\n",
    "     * Promotional spikes at specific prices\n",
    "     * Low-stock periods that force higher-price sales\n",
    "     * Data anomalies (returns, refunds misâ€‘recorded)\n",
    "\n",
    "4. **Weak Model Fit (Low RÂ² Values)**\n",
    "\n",
    "   * Except for Saree (RÂ² â‰ˆ 0.05), all RÂ² values are very low (< 0.05).\n",
    "   * **Meaning**: Price alone explains only a tiny fraction of demand variation at the category level.\n",
    "   * **Next Steps**: Incorporate additional features (seasonality, promotions, channel mix) or switch to **nonâ€‘linear**/segmented models.\n",
    "\n",
    "---\n",
    "\n",
    "## Strategic Takeaways\n",
    "\n",
    "| Focus Area               | Recommended Next Steps                                                                             |\n",
    "| ------------------------ | -------------------------------------------------------------------------------------------------- |\n",
    "| **Inelastic Categories** | Test **small price increases** (e.g. +5â€“10%) and monitor volume impact. Focus on margin expansion. |\n",
    "| **Positive Elasticity**  | Audit data and business context. Consider **premium positioning** or adjust for anomalies.         |\n",
    "| **Model Improvement**    | Add features (e.g. seasonal dummies, channel flags) or try **piecewise / polynomial** fits.        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7b0ac",
   "metadata": {},
   "source": [
    "Starting Price Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = (\n",
    "    shipped_df\n",
    "    .groupby('category')\n",
    "    .agg(\n",
    "        baseline_qty   = ('qty', 'sum'),\n",
    "        baseline_rev   = ('unit_price', lambda s: (s * shipped_df.loc[s.index, 'qty']).sum())\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f778cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge elasticity into baseline\n",
    "baseline = baseline.merge(category_elasticity_df[['category','elasticity']], on='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9136cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define price adjustment multipliers (âˆ’10% to +10% in 5% steps)\n",
    "multipliers = np.linspace(0.90, 1.10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ec1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Build a scenario table\n",
    "rows = []\n",
    "for _, row in baseline.iterrows():\n",
    "    cat, Q0, R0, E = row['category'], row['baseline_qty'], row['baseline_rev'], row['elasticity']\n",
    "    for m in multipliers:\n",
    "        P_mult = m\n",
    "        # Forecast new quantity: Q1 = Q0 * (P_mult)^E\n",
    "        Q1 = Q0 * (P_mult ** E)\n",
    "        # Forecast new revenue: R1 = R0 * P_mult * (P_mult^E)\n",
    "        R1 = R0 * P_mult * (P_mult ** E)\n",
    "        rows.append({\n",
    "            'category':          cat,\n",
    "            'price_change_pct':  (m - 1)*100,\n",
    "            'baseline_revenue':  R0,\n",
    "            'simulated_revenue': R1,\n",
    "            'rev_delta_pct':     (R1 - R0) / R0 * 100\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066033fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scenario_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf395640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Pivot for readability (categories as rows, price_change_pct as columns)\n",
    "pivot = scenario_df.pivot_table(\n",
    "    index='category',\n",
    "    columns='price_change_pct',\n",
    "    values='rev_delta_pct'\n",
    ").round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Revenue % Change vs Baseline (by Category):\")\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4a8e1",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Revenue Gains from Price Increases**\n",
    "\n",
    "   * **Western Dress** sees the largest upside:\n",
    "\n",
    "     * +5% price â†’ +11.1% revenue\n",
    "     * +10% price â†’ +22.9% revenue\n",
    "   * **Blouse** and **Top** also benefit significantly from moderate price hikes:\n",
    "\n",
    "     * Blouse: +10% price â†’ +18.3% revenue\n",
    "     * Top:   +10% price â†’ +13.2% revenue\n",
    "\n",
    "2. **Limited Impact of Price Cuts**\n",
    "\n",
    "   * Across all categories, **cutting prices reduces revenue**.\n",
    "   * Even the â€œelasticâ€ categories (Blouse, Top, Western Dress) donâ€™t recoup enough volume to offset the lower price.\n",
    "   * **Conclusion**: Across the board, **price increases** are more beneficial than discounts.\n",
    "\n",
    "3. **Near Unitary Category (Saree)**\n",
    "\n",
    "   * **Saree** shows essentially **0% change** under Â±10% moves.\n",
    "   * Elasticity â‰ˆ â€“1 â†’ **unit elastic**:\n",
    "\n",
    "     * Price hikes or cuts leave revenue roughly unchanged.\n",
    "\n",
    "4. **Mild Sensitivity Categories**\n",
    "\n",
    "   * **Kurta** and **Bottom**:\n",
    "\n",
    "     * +10% price â†’ +3.9% (Kurta) and +6.1% (Bottom) revenue gain\n",
    "   * **Ethnic Dress** and **Set**:\n",
    "\n",
    "     * +10% price â†’ \\~+9â€“8% revenue gain\n",
    "\n",
    "---\n",
    "\n",
    "## Strategic Recommendations\n",
    "\n",
    "1. **Implement Price Increases**\n",
    "\n",
    "   * **Western Dress**, **Blouse**, **Top**: Raise prices by +5â€“10% to boost revenue.\n",
    "2. **Marginal Adjustments for Midâ€‘Sensitivity SKUs**\n",
    "\n",
    "   * **Ethnic Dress**, **Bottom**, **Kurta**, **Set**: Consider modest price hikes (+5%) for incremental revenue.\n",
    "3. **Maintain Current Pricing on Sarees**\n",
    "\n",
    "   * Since revenue is flat, focus on other levers (bundles, promotions) rather than price.\n",
    "4. **Avoid Acrossâ€‘theâ€‘Board Discounts**\n",
    "\n",
    "   * Even â€œelasticâ€ segments didnâ€™t generate enough volume to justify price cuts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_or_nan(series):\n",
    "    \"\"\"Return the mode of the non-null values in series, or np.nan if none exist.\"\"\"\n",
    "    vals = series.dropna()\n",
    "    return vals.mode().iat[0] if not vals.mode().empty else np.nan\n",
    "\n",
    "sku_meta = (\n",
    "    shipped_df\n",
    "      .groupby('sku')\n",
    "      .agg(\n",
    "          sales_channel = ('sales_channel', most_common_or_nan),\n",
    "          fulfilled_by  = ('fulfilled_by',  most_common_or_nan),\n",
    "          b2b_flag      = ('b2b',           most_common_or_nan),\n",
    "      )\n",
    "      .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a14800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_df = elasticity_df.merge(sku_meta, on='sku', how='left')\n",
    "seg_df = seg_df.dropna(subset=['sales_channel','fulfilled_by','b2b_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54adcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the dimensions to segment by\n",
    "segment_cols = ['sales_channel', 'fulfilled_by', 'b2b_flag']\n",
    "\n",
    "# for each dimension, get mean/median/count of elasticity\n",
    "segment_stats = {}\n",
    "for col in segment_cols:\n",
    "    stats = (\n",
    "        seg_df\n",
    "        .groupby(col)['elasticity']\n",
    "        .agg(['count', 'mean', 'median'])\n",
    "        .rename(columns={'count':'n_skus','mean':'avg_elasticity','median':'med_elasticity'})\n",
    "        .sort_values('avg_elasticity')\n",
    "    )\n",
    "    segment_stats[col] = stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f62d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, stats in segment_stats.items():\n",
    "    print(f\"\\n=== Segment: {col} ===\")\n",
    "    print(stats.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12abc8a",
   "metadata": {},
   "source": [
    "Since every SKU in your sample falls under the **same** metadata values, the segmentation returned only one group for each dimension:\n",
    "\n",
    "* **Sales Channel**: all SKUs are sold via `amazon.in`\n",
    "* **Fulfillment**: all are fulfilled by `easy ship`\n",
    "* **B2B Flag**: all are retail (`False`)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Means\n",
    "\n",
    "1. **No Variability in These Dimensions**\n",
    "\n",
    "   * Because our dataset (or the subset you ran this on) only contains a single sales channel, fulfillment type, and B2B flag, you canâ€™t learn anything about how elasticity varies across these segmentsâ€”theyâ€™re all the same.\n",
    "\n",
    "2. **Your Segmentâ€‘Level Averages Mirror the Full Catalog**\n",
    "\n",
    "   * `avg_elasticity = 1.49` and `med_elasticity = â€“0.77` simply restate your overall SKU elasticity distribution (skewed positive mean, negative median) for the entire set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952d2d28",
   "metadata": {},
   "source": [
    "Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0513507",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = (\n",
    "    shipped_df\n",
    "    .assign(revenue = lambda df: df['unit_price'] * df['qty'])\n",
    "    .groupby('category')\n",
    "    .agg(\n",
    "        baseline_qty = ('qty', 'sum'),\n",
    "        baseline_rev = ('revenue', 'sum')\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aedd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in the elasticity coefficients we computed earlier\n",
    "baseline = baseline.merge(category_elasticity_df[['category','elasticity']], on='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a01fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 2. Define Scenario Parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "disc_multipliers = [0.90, 0.80]     # â€“10% and â€“20% discount\n",
    "peak_uplifts     = [1.10, 1.20]     # +10% and +20% baseline demand uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e39306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 3. Promotion Scenarios (Price Discount) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "promo_rows = []\n",
    "for _, row in baseline.iterrows():\n",
    "    cat, Q0, R0, E = row['category'], row['baseline_qty'], row['baseline_rev'], row['elasticity']\n",
    "    for m in disc_multipliers:\n",
    "        # Price is discounted â†’ unit_price * m\n",
    "        # New quantity: Q1 = Q0 * (m ** E)\n",
    "        Q1 = Q0 * (m ** E)\n",
    "        # New revenue: R1 = (R0 / Q0 * m) * Q1  ==> baseline unit price * m * Q1\n",
    "        R1 = (R0 / Q0) * m * Q1\n",
    "        promo_rows.append({\n",
    "            'category':         cat,\n",
    "            'scenario':         f'{int((m-1)*100)}% discount',\n",
    "            'baseline_rev':     R0,\n",
    "            'sim_rev':          R1,\n",
    "            'rev_change_pct':   (R1 - R0) / R0 * 100\n",
    "        })\n",
    "\n",
    "promo_df = pd.DataFrame(promo_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7c9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€ 4. Peakâ€‘Demand Scenarios (Volume Uplift) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "peak_rows = []\n",
    "for _, row in baseline.iterrows():\n",
    "    cat, Q0, R0 = row['category'], row['baseline_qty'], row['baseline_rev']\n",
    "    for u in peak_uplifts:\n",
    "        # Assume price stays same, but quantity increases by uplift factor\n",
    "        Q1 = Q0 * u\n",
    "        R1 = R0 * u\n",
    "        peak_rows.append({\n",
    "            'category':       cat,\n",
    "            'scenario':       f'{int((u-1)*100)}% demand uplift',\n",
    "            'baseline_rev':   R0,\n",
    "            'sim_rev':        R1,\n",
    "            'rev_change_pct': (R1 - R0) / R0 * 100\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23402c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df = pd.DataFrame(peak_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fdf905",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_df = pd.concat([promo_df, peak_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c12bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = evaluation_df.pivot_table(\n",
    "    index='category',\n",
    "    columns='scenario',\n",
    "    values='rev_change_pct'\n",
    ").round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34513dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49895e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot from your last step: pivot (categories Ã— scenarios) of rev_change_pct\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    pivot, \n",
    "    annot=True, \n",
    "    fmt=\".1f\", \n",
    "    cmap=\"RdYlGn\", \n",
    "    center=0,\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title(\"Revenue % Change vs Baseline (Categories Ã— Scenarios)\")\n",
    "plt.xlabel(\"Scenario\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260db9f5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
